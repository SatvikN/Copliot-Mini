["class CodeCompletionRequest(BaseModel):\n    code: str\n    language: str\n    cursor_position: Optional[int] = None\n    max_suggestions: Optional[int] = 3\n    context_lines: Optional[int] = 10", "class CodeCompletionResponse(BaseModel):\n    suggestions: List[str]\n    confidence_scores: List[float]\n    processing_time_ms: float\n    model_used: str", "class ChatRequest(BaseModel):\n    message: str\n    code_context: Optional[str] = None\n    language: Optional[str] = None", "class ChatResponse(BaseModel):\n    response: str\n    code_suggestions: Optional[List[str]] = None\n    processing_time_ms: float", "class HealthResponse(BaseModel):\n    status: str\n    timestamp: str\n    version: str\n    active_connections: int\n    model_status: str", "async def root():\n    \"\"\"Root endpoint with basic API information.\"\"\"\n    return {\n        \"name\": \"CopilotMini API\",\n        \"version\": \"0.1.0\",\n        \"status\": \"running\",\n        \"docs\": \"/docs\",\n        \"websocket\": \"/ws\"\n    }", "async def health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    return HealthResponse(\n        status=\"healthy\",\n        timestamp=datetime.now().isoformat(),\n        version=\"0.1.0\",\n        active_connections=len(connection_manager.active_connections),\n        model_status=inference_engine.get_status()\n    )", "async def complete_code(request: CodeCompletionRequest):\n    \"\"\"Generate code completions for the given input.\"\"\"\n    try:\n        start_time = asyncio.get_event_loop().time()\n        # RAG: Retrieve relevant context\n        rag_results = rag_retriever.retrieve(request.code, top_k=3)\n        rag_context = '\\n\\n'.join([r['text'] for r in rag_results])\n        # Prepend RAG context to code\n        code_with_context = f\"# Relevant context from project:\\n{rag_context}\\n\\n{request.code}\"\n        # Generate completions using inference engine\n        result = await inference_engine.generate_completions(\n            code=code_with_context,\n            language=request.language,\n            max_suggestions=request.max_suggestions or 3,\n            cursor_position=request.cursor_position\n        )\n        end_time = asyncio.get_event_loop().time()\n        processing_time = (end_time - start_time) * 1000  # Convert to milliseconds\n        return CodeCompletionResponse(\n            suggestions=result[\"suggestions\"],\n            confidence_scores=result[\"confidence_scores\"],\n            processing_time_ms=processing_time,\n            model_used=result[\"model_used\"]\n        )\n    except Exception as e:\n        logger.error(f\"Error in code completion: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))", "async def chat_with_ai(request: ChatRequest):\n    \"\"\"Chat with AI about code.\"\"\"\n    try:\n        start_time = asyncio.get_event_loop().time()\n        # RAG: Retrieve relevant context\n        rag_results = rag_retriever.retrieve(request.message, top_k=3)\n        rag_context = '\\n\\n'.join([r['text'] for r in rag_results])\n        # Prepend RAG context to code_context if present, else just use RAG\n        if request.code_context:\n            code_context_with_rag = f\"# Relevant context from project:\\n{rag_context}\\n\\n{request.code_context}\"\n        else:\n            code_context_with_rag = f\"# Relevant context from project:\\n{rag_context}\"\n        # Process chat request\n        result = await inference_engine.process_chat(\n            message=request.message,\n            code_context=code_context_with_rag,\n            language=request.language\n        )\n        end_time = asyncio.get_event_loop().time()\n        processing_time = (end_time - start_time) * 1000\n        return ChatResponse(\n            response=result[\"response\"],\n            code_suggestions=result.get(\"code_suggestions\"),\n            processing_time_ms=processing_time\n        )\n    except Exception as e:\n        logger.error(f\"Error in chat processing: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))", "async def get_stats():\n    \"\"\"Get API usage statistics.\"\"\"\n    return {\n        \"active_connections\": len(connection_manager.active_connections),\n        \"total_requests\": 0,  # TODO: Implement request counting\n        \"model_info\": inference_engine.get_model_info(),\n        \"uptime\": \"0 minutes\"  # TODO: Implement uptime tracking\n    }", "async def get_engines_status():\n    \"\"\"Get status of all available inference engines.\"\"\"\n    status = {\n        \"current_engine\": type(inference_engine).__name__,\n        \"available_engines\": {},\n        \"environment\": {\n            \"USE_REAL_AI\": USE_REAL_AI,\n            \"USE_CUSTOM_AI\": USE_CUSTOM_AI\n        }\n    }\n    \n    # Check mock engine (always available)\n    status[\"available_engines\"][\"mock\"] = {\n        \"available\": True,\n        \"status\": \"ready\",\n        \"description\": \"Mock inference engine for development\"\n    }\n    \n    # Check real AI engine\n    if REAL_AI_AVAILABLE:\n        real_engine = RealInferenceEngine()\n        status[\"available_engines\"][\"real_ai\"] = {\n            \"available\": True,\n            \"status\": real_engine.get_status(),\n            \"description\": \"Real AI models (OpenAI, Ollama, HuggingFace)\"\n        }\n    else:\n        status[\"available_engines\"][\"real_ai\"] = {\n            \"available\": False,\n            \"status\": \"not_available\",\n            \"description\": \"Real AI dependencies not installed\"\n        }\n    \n    # Check custom AI engine\n    if CUSTOM_AI_AVAILABLE:\n        custom_engine = CustomInferenceEngine()\n        try:\n            custom_initialized = await custom_engine.initialize()\n            if custom_initialized:\n                model_info = custom_engine.get_model_info()\n                status[\"available_engines\"][\"custom_ai\"] = {\n                    \"available\": True,\n                    \"status\": \"ready\",\n                    \"description\": f\"Custom fine-tuned models ({model_info['total_models']} models available)\",\n                    \"models\": model_info.get(\"models\", [])\n                }\n                await custom_engine.cleanup()\n            else:\n                status[\"available_engines\"][\"custom_ai\"] = {\n                    \"available\": False,\n                    \"status\": \"no_models\",\n                    \"description\": \"No trained custom models found\"\n                }\n        except Exception as e:\n            status[\"available_engines\"][\"custom_ai\"] = {\n                \"available\": False,\n                \"status\": \"error\",\n                \"description\": f\"Custom AI initialization failed: {str(e)}\"\n            }\n    else:\n        status[\"available_engines\"][\"custom_ai\"] = {\n            \"available\": False,\n            \"status\": \"not_available\",\n            \"description\": \"Custom AI engine not available\"\n        }\n    \n    return status", "async def websocket_endpoint(websocket: WebSocket):\n    \"\"\"WebSocket endpoint for real-time code completion.\"\"\"\n    await connection_manager.connect(websocket)\n    logger.info(f\"WebSocket connection established. Active connections: {len(connection_manager.active_connections)}\")\n    \n    try:\n        while True:\n            # Receive message from client\n            data = await websocket.receive_text()\n            message = json.loads(data)\n            \n            # Process different message types\n            if message.get(\"type\") == \"completion_request\":\n                await handle_completion_request(websocket, message)\n            elif message.get(\"type\") == \"chat_request\":\n                await handle_chat_request(websocket, message)\n            elif message.get(\"type\") == \"ping\":\n                await websocket.send_text(json.dumps({\"type\": \"pong\", \"timestamp\": datetime.now().isoformat()}))\n            else:\n                await websocket.send_text(json.dumps({\n                    \"type\": \"error\",\n                    \"message\": f\"Unknown message type: {message.get('type')}\"\n                }))\n                \n    except WebSocketDisconnect:\n        connection_manager.disconnect(websocket)\n        logger.info(f\"WebSocket connection closed. Active connections: {len(connection_manager.active_connections)}\")\n    except Exception as e:\n        logger.error(f\"WebSocket error: {e}\")\n        connection_manager.disconnect(websocket)", "async def handle_completion_request(websocket: WebSocket, message: Dict[str, Any]):\n    \"\"\"Handle code completion request via WebSocket.\"\"\"\n    try:\n        # Extract request data\n        code = message.get(\"code\", \"\")\n        language = message.get(\"language\", \"python\")\n        max_suggestions = message.get(\"max_suggestions\", 3)\n        cursor_position = message.get(\"cursor_position\")\n        \n        # Generate completions\n        result = await inference_engine.generate_completions(\n            code=code,\n            language=language,\n            max_suggestions=max_suggestions,\n            cursor_position=cursor_position\n        )\n        \n        # Send response\n        response = {\n            \"type\": \"completion_response\",\n            \"request_id\": message.get(\"request_id\"),\n            \"suggestions\": result[\"suggestions\"],\n            \"confidence_scores\": result[\"confidence_scores\"],\n            \"model_used\": result[\"model_used\"]\n        }\n        \n        await websocket.send_text(json.dumps(response))\n        \n    except Exception as e:\n        logger.error(f\"Error handling completion request: {e}\")\n        error_response = {\n            \"type\": \"error\",\n            \"request_id\": message.get(\"request_id\"),\n            \"message\": str(e)\n        }\n        await websocket.send_text(json.dumps(error_response))", "async def handle_chat_request(websocket: WebSocket, message: Dict[str, Any]):\n    \"\"\"Handle chat request via WebSocket.\"\"\"\n    try:\n        # Extract request data\n        user_message = message.get(\"message\", \"\")\n        code_context = message.get(\"code_context\")\n        language = message.get(\"language\")\n        \n        # Process chat\n        result = await inference_engine.process_chat(\n            message=user_message,\n            code_context=code_context,\n            language=language\n        )\n        \n        # Send response\n        response = {\n            \"type\": \"chat_response\",\n            \"request_id\": message.get(\"request_id\"),\n            \"response\": result[\"response\"],\n            \"code_suggestions\": result.get(\"code_suggestions\")\n        }\n        \n        await websocket.send_text(json.dumps(response))\n        \n    except Exception as e:\n        logger.error(f\"Error handling chat request: {e}\")\n        error_response = {\n            \"type\": \"error\",\n            \"request_id\": message.get(\"request_id\"),\n            \"message\": str(e)\n        }\n        await websocket.send_text(json.dumps(error_response))", "async def startup_event():\n    \"\"\"Initialize services on startup.\"\"\"\n    logger.info(\"Starting CopilotMini API server...\")\n    \n    # Initialize inference engine\n    await inference_engine.initialize()\n    logger.info(\"Inference engine initialized\")\n    \n    logger.info(f\"Server started on {API_CONFIG['host']}:{API_CONFIG['port']}\")", "async def shutdown_event():\n    \"\"\"Cleanup on shutdown.\"\"\"\n    logger.info(\"Shutting down CopilotMini API server...\")\n    \n    # Cleanup inference engine\n    await inference_engine.cleanup()\n    \n    # Close all WebSocket connections\n    await connection_manager.disconnect_all()\n    \n    logger.info(\"Server shutdown complete\")", "class ConnectionManager:\n    \"\"\"Manages WebSocket connections for real-time communication.\"\"\"\n    \n    def __init__(self):\n        self.active_connections: List[WebSocket] = []\n        self.connection_info: Dict[WebSocket, Dict[str, Any]] = {}\n    \n    async def connect(self, websocket: WebSocket):\n        \"\"\"Accept a new WebSocket connection.\"\"\"\n        await websocket.accept()\n        self.active_connections.append(websocket)\n        \n        # Store connection metadata\n        self.connection_info[websocket] = {\n            \"connected_at\": asyncio.get_event_loop().time(),\n            \"client_info\": websocket.client,\n            \"messages_sent\": 0,\n            \"messages_received\": 0\n        }\n        \n        logger.info(f\"New WebSocket connection from {websocket.client}\")\n    \n    def disconnect(self, websocket: WebSocket):\n        \"\"\"Remove a WebSocket connection.\"\"\"\n        if websocket in self.active_connections:\n            self.active_connections.remove(websocket)\n            \n        if websocket in self.connection_info:\n            connection_time = asyncio.get_event_loop().time() - self.connection_info[websocket][\"connected_at\"]\n            logger.info(f\"WebSocket disconnected after {connection_time:.2f}s. Messages: \"\n                       f\"sent={self.connection_info[websocket]['messages_sent']}, \"\n                       f\"received={self.connection_info[websocket]['messages_received']}\")\n            del self.connection_info[websocket]\n    \n    async def send_personal_message(self, message: str, websocket: WebSocket):\n        \"\"\"Send a message to a specific WebSocket connection.\"\"\"\n        try:\n            await websocket.send_text(message)\n            if websocket in self.connection_info:\n                self.connection_info[websocket][\"messages_sent\"] += 1\n        except Exception as e:\n            logger.error(f\"Error sending message to WebSocket: {e}\")\n            self.disconnect(websocket)\n    \n    async def send_personal_json(self, data: Dict[str, Any], websocket: WebSocket):\n        \"\"\"Send JSON data to a specific WebSocket connection.\"\"\"\n        try:\n            await websocket.send_text(json.dumps(data))\n            if websocket in self.connection_info:\n                self.connection_info[websocket][\"messages_sent\"] += 1\n        except Exception as e:\n            logger.error(f\"Error sending JSON to WebSocket: {e}\")\n            self.disconnect(websocket)\n    \n    async def broadcast(self, message: str):\n        \"\"\"Broadcast a message to all active connections.\"\"\"\n        if not self.active_connections:\n            return\n        \n        disconnected = []\n        for connection in self.active_connections:\n            try:\n                await connection.send_text(message)\n                if connection in self.connection_info:\n                    self.connection_info[connection][\"messages_sent\"] += 1\n            except Exception as e:\n                logger.error(f\"Error broadcasting to WebSocket: {e}\")\n                disconnected.append(connection)\n        \n        # Clean up disconnected connections\n        for connection in disconnected:\n            self.disconnect(connection)\n    \n    async def broadcast_json(self, data: Dict[str, Any]):\n        \"\"\"Broadcast JSON data to all active connections.\"\"\"\n        await self.broadcast(json.dumps(data))\n    \n    async def disconnect_all(self):\n        \"\"\"Disconnect all active connections.\"\"\"\n        for connection in self.active_connections.copy():\n            try:\n                await connection.close()\n            except Exception as e:\n                logger.error(f\"Error closing WebSocket connection: {e}\")\n            finally:\n                self.disconnect(connection)\n    \n    def get_connection_stats(self) -> Dict[str, Any]:\n        \"\"\"Get statistics about active connections.\"\"\"\n        total_messages_sent = sum(info[\"messages_sent\"] for info in self.connection_info.values())\n        total_messages_received = sum(info[\"messages_received\"] for info in self.connection_info.values())\n        \n        return {\n            \"active_connections\": len(self.active_connections),\n            \"total_messages_sent\": total_messages_sent,\n            \"total_messages_received\": total_messages_received,\n            \"connections\": [\n                {\n                    \"client\": str(ws.client),\n                    \"connected_at\": info[\"connected_at\"],\n                    \"messages_sent\": info[\"messages_sent\"],\n                    \"messages_received\": info[\"messages_received\"]\n                }\n                for ws, info in self.connection_info.items()\n            ]\n        }\n    \n    def track_received_message(self, websocket: WebSocket):\n        \"\"\"Track that a message was received from a WebSocket.\"\"\"\n        if websocket in self.connection_info:\n            self.connection_info[websocket][\"messages_received\"] += 1 ", "    def __init__(self):\n        self.active_connections: List[WebSocket] = []\n        self.connection_info: Dict[WebSocket, Dict[str, Any]] = {}", "    async def connect(self, websocket: WebSocket):\n        \"\"\"Accept a new WebSocket connection.\"\"\"\n        await websocket.accept()\n        self.active_connections.append(websocket)\n        \n        # Store connection metadata\n        self.connection_info[websocket] = {\n            \"connected_at\": asyncio.get_event_loop().time(),\n            \"client_info\": websocket.client,\n            \"messages_sent\": 0,\n            \"messages_received\": 0\n        }\n        \n        logger.info(f\"New WebSocket connection from {websocket.client}\")", "    def disconnect(self, websocket: WebSocket):\n        \"\"\"Remove a WebSocket connection.\"\"\"\n        if websocket in self.active_connections:\n            self.active_connections.remove(websocket)\n            \n        if websocket in self.connection_info:\n            connection_time = asyncio.get_event_loop().time() - self.connection_info[websocket][\"connected_at\"]\n            logger.info(f\"WebSocket disconnected after {connection_time:.2f}s. Messages: \"\n                       f\"sent={self.connection_info[websocket]['messages_sent']}, \"\n                       f\"received={self.connection_info[websocket]['messages_received']}\")\n            del self.connection_info[websocket]", "    async def send_personal_message(self, message: str, websocket: WebSocket):\n        \"\"\"Send a message to a specific WebSocket connection.\"\"\"\n        try:\n            await websocket.send_text(message)\n            if websocket in self.connection_info:\n                self.connection_info[websocket][\"messages_sent\"] += 1\n        except Exception as e:\n            logger.error(f\"Error sending message to WebSocket: {e}\")\n            self.disconnect(websocket)", "    async def send_personal_json(self, data: Dict[str, Any], websocket: WebSocket):\n        \"\"\"Send JSON data to a specific WebSocket connection.\"\"\"\n        try:\n            await websocket.send_text(json.dumps(data))\n            if websocket in self.connection_info:\n                self.connection_info[websocket][\"messages_sent\"] += 1\n        except Exception as e:\n            logger.error(f\"Error sending JSON to WebSocket: {e}\")\n            self.disconnect(websocket)", "    async def broadcast(self, message: str):\n        \"\"\"Broadcast a message to all active connections.\"\"\"\n        if not self.active_connections:\n            return\n        \n        disconnected = []\n        for connection in self.active_connections:\n            try:\n                await connection.send_text(message)\n                if connection in self.connection_info:\n                    self.connection_info[connection][\"messages_sent\"] += 1\n            except Exception as e:\n                logger.error(f\"Error broadcasting to WebSocket: {e}\")\n                disconnected.append(connection)\n        \n        # Clean up disconnected connections\n        for connection in disconnected:\n            self.disconnect(connection)", "    async def broadcast_json(self, data: Dict[str, Any]):\n        \"\"\"Broadcast JSON data to all active connections.\"\"\"\n        await self.broadcast(json.dumps(data))", "    async def disconnect_all(self):\n        \"\"\"Disconnect all active connections.\"\"\"\n        for connection in self.active_connections.copy():\n            try:\n                await connection.close()\n            except Exception as e:\n                logger.error(f\"Error closing WebSocket connection: {e}\")\n            finally:\n                self.disconnect(connection)", "    def get_connection_stats(self) -> Dict[str, Any]:\n        \"\"\"Get statistics about active connections.\"\"\"\n        total_messages_sent = sum(info[\"messages_sent\"] for info in self.connection_info.values())\n        total_messages_received = sum(info[\"messages_received\"] for info in self.connection_info.values())\n        \n        return {\n            \"active_connections\": len(self.active_connections),\n            \"total_messages_sent\": total_messages_sent,\n            \"total_messages_received\": total_messages_received,\n            \"connections\": [\n                {\n                    \"client\": str(ws.client),\n                    \"connected_at\": info[\"connected_at\"],\n                    \"messages_sent\": info[\"messages_sent\"],\n                    \"messages_received\": info[\"messages_received\"]\n                }\n                for ws, info in self.connection_info.items()\n            ]\n        }", "    def track_received_message(self, websocket: WebSocket):\n        \"\"\"Track that a message was received from a WebSocket.\"\"\"\n        if websocket in self.connection_info:\n            self.connection_info[websocket][\"messages_received\"] += 1 ", "class RealInferenceEngine:\n    \"\"\"Real AI inference engine using OpenAI GPT-4 or local models\"\"\"\n    \n    def __init__(self):\n        self.client = None\n        self.model_type = \"openai\"  # or \"ollama\", \"huggingface\"\n        self.model_name = \"gpt-4\"\n        self.is_initialized = False\n        \n    async def initialize(self) -> bool:\n        \"\"\"Initialize the real AI inference engine\"\"\"\n        try:\n            # Try OpenAI first\n            if await self._try_openai():\n                self.model_type = \"openai\"\n                logger.info(\"\u2705 OpenAI GPT-4 initialized successfully\")\n                self.is_initialized = True\n                return True\n            \n            # Fallback to Ollama\n            if await self._try_ollama():\n                self.model_type = \"ollama\"\n                logger.info(\"\u2705 Ollama local model initialized successfully\")\n                self.is_initialized = True\n                return True\n            \n            # Fallback to HuggingFace\n            if await self._try_huggingface():\n                self.model_type = \"huggingface\"\n                logger.info(\"\u2705 HuggingFace model initialized successfully\")\n                self.is_initialized = True\n                return True\n            \n            logger.warning(\"\u26a0\ufe0f No real AI models available, falling back to mock\")\n            return False\n            \n        except Exception as e:\n            logger.error(f\"Failed to initialize real inference engine: {e}\")\n            return False\n    \n    async def _try_openai(self) -> bool:\n        \"\"\"Try to initialize OpenAI\"\"\"\n        try:\n            api_key = os.getenv(\"OPENAI_API_KEY\")\n            if not api_key:\n                logger.info(\"No OPENAI_API_KEY found, skipping OpenAI\")\n                return False\n            \n            import openai\n            self.client = openai.AsyncOpenAI(api_key=api_key)\n            \n            # Test the connection\n            response = await self.client.chat.completions.create(\n                model=\"gpt-4\",\n                messages=[{\"role\": \"user\", \"content\": \"test\"}],\n                max_tokens=5\n            )\n            return True\n            \n        except ImportError:\n            logger.info(\"OpenAI package not installed: pip install openai\")\n            return False\n        except Exception as e:\n            logger.info(f\"OpenAI initialization failed: {e}\")\n            return False\n    \n    async def _try_ollama(self) -> bool:\n        \"\"\"Try to initialize Ollama local models\"\"\"\n        try:\n            import aiohttp\n            \n            # Check if Ollama is running\n            async with aiohttp.ClientSession() as session:\n                async with session.get(\"http://localhost:11434/api/tags\") as response:\n                    if response.status == 200:\n                        models = await response.json()\n                        available_models = [m[\"name\"] for m in models.get(\"models\", [])]\n                        \n                        # Prefer code-specific models\n                        preferred_models = [\"codellama:7b\", \"starcoder:3b\", \"codellama:13b\", \"llama2:7b\"]\n                        for model in preferred_models:\n                            if model in available_models:\n                                self.model_name = model\n                                logger.info(f\"Using Ollama model: {model}\")\n                                return True\n                        \n                        if available_models:\n                            self.model_name = available_models[0]\n                            logger.info(f\"Using Ollama model: {self.model_name}\")\n                            return True\n            \n            return False\n            \n        except ImportError:\n            logger.info(\"aiohttp package needed for Ollama: pip install aiohttp\")\n            return False\n        except Exception as e:\n            logger.info(f\"Ollama not available: {e}\")\n            return False\n    \n    async def _try_huggingface(self) -> bool:\n        \"\"\"Try to initialize HuggingFace models\"\"\"\n        try:\n            from transformers import AutoModelForCausalLM, AutoTokenizer\n            import torch\n            \n            # Use a small, fast model for MVP\n            model_name = \"microsoft/DialoGPT-small\"  # Fast model for testing\n            \n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n            self.model = AutoModelForCausalLM.from_pretrained(model_name)\n            self.model_name = model_name\n            \n            # Add padding token if missing\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            logger.info(f\"Loaded HuggingFace model: {model_name}\")\n            return True\n            \n        except ImportError:\n            logger.info(\"HuggingFace transformers not installed: pip install transformers torch\")\n            return False\n        except Exception as e:\n            logger.info(f\"HuggingFace model loading failed: {e}\")\n            return False\n    \n    async def generate_completion(\n        self,\n        code: str,\n        language: str,\n        max_length: int = 100,\n        num_suggestions: int = 3\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Generate code completions using real AI models\"\"\"\n        \n        if not self.is_initialized:\n            return await self._fallback_mock_completion(code, language, num_suggestions)\n        \n        try:\n            start_time = time.time()\n            \n            if self.model_type == \"openai\":\n                suggestions = await self._openai_completion(code, language, num_suggestions)\n            elif self.model_type == \"ollama\":\n                suggestions = await self._ollama_completion(code, language, num_suggestions)\n            elif self.model_type == \"huggingface\":\n                suggestions = await self._huggingface_completion(code, language, num_suggestions)\n            else:\n                suggestions = await self._fallback_mock_completion(code, language, num_suggestions)\n            \n            processing_time = time.time() - start_time\n            \n            # Add metadata to suggestions\n            for suggestion in suggestions:\n                suggestion[\"processing_time_ms\"] = round(processing_time * 1000, 2)\n                suggestion[\"model_used\"] = f\"{self.model_type}:{self.model_name}\"\n            \n            return suggestions[:num_suggestions]\n            \n        except Exception as e:\n            logger.error(f\"Real inference failed: {e}\")\n            return await self._fallback_mock_completion(code, language, num_suggestions)\n    \n    async def _openai_completion(self, code: str, language: str, num_suggestions: int) -> List[Dict[str, Any]]:\n        \"\"\"Generate completions using OpenAI GPT-4. The 'code' argument may already include RAG context prepended.\"\"\"\n        \n        # Create a focused prompt for code completion\n        prompt = f\"\"\"You are an expert {language} programmer. Complete the following code with {num_suggestions} different suggestions.\nOnly return the completion code without explanations.\n\nCode to complete:\n```{language}\n{code}\n```\n\nComplete this code with the most likely continuation:\"\"\"\n\n        response = await self.client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a code completion AI. Only return code completions, no explanations.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            max_tokens=150,\n            temperature=0.3,\n            n=num_suggestions\n        )\n        \n        suggestions = []\n        for i, choice in enumerate(response.choices):\n            completion = choice.message.content.strip()\n            # Clean up the completion\n            if \"```\" in completion:\n                completion = completion.split(\"```\")[1].strip()\n                if completion.startswith(language):\n                    completion = completion[len(language):].strip()\n            \n            suggestions.append({\n                \"text\": completion,\n                \"confidence\": 0.85 + (i * 0.05),  # Decreasing confidence\n                \"source\": \"gpt-4\"\n            })\n        \n        return suggestions\n    \n    async def _ollama_completion(self, code: str, language: str, num_suggestions: int) -> List[Dict[str, Any]]:\n        \"\"\"Generate completions using Ollama local models. The 'code' argument may already include RAG context prepended.\"\"\"\n        import aiohttp\n        \n        prompt = f\"Complete this {language} code:\\n{code}\"\n        \n        suggestions = []\n        async with aiohttp.ClientSession() as session:\n            for i in range(num_suggestions):\n                payload = {\n                    \"model\": self.model_name,\n                    \"prompt\": prompt,\n                    \"stream\": False,\n                    \"options\": {\n                        \"temperature\": 0.3 + (i * 0.2),  # Vary temperature for diversity\n                        \"top_p\": 0.9,\n                        \"max_tokens\": 100\n                    }\n                }\n                \n                async with session.post(\"http://localhost:11434/api/generate\", json=payload) as response:\n                    if response.status == 200:\n                        result = await response.json()\n                        completion = result.get(\"response\", \"\").strip()\n                        \n                        suggestions.append({\n                            \"text\": completion,\n                            \"confidence\": 0.80 - (i * 0.1),\n                            \"source\": self.model_name\n                        })\n        \n        return suggestions\n    \n    async def _huggingface_completion(self, code: str, language: str, num_suggestions: int) -> List[Dict[str, Any]]:\n        \"\"\"Generate completions using HuggingFace models. The 'code' argument may already include RAG context prepended.\"\"\"\n        import torch\n        \n        # Prepare input\n        prompt = f\"# {language} code completion\\n{code}\"\n        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n        \n        suggestions = []\n        for i in range(num_suggestions):\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    inputs,\n                    max_length=inputs.shape[1] + 50,\n                    temperature=0.7 + (i * 0.2),\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.eos_token_id\n                )\n            \n            # Decode completion\n            completion = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n            \n            suggestions.append({\n                \"text\": completion.strip(),\n                \"confidence\": 0.75 - (i * 0.1),\n                \"source\": self.model_name\n            })\n        \n        return suggestions\n    \n    async def _fallback_mock_completion(self, code: str, language: str, num_suggestions: int) -> List[Dict[str, Any]]:\n        \"\"\"Fallback to mock completions if real AI fails\"\"\"\n        from .inference import MockInferenceEngine\n        \n        mock_engine = MockInferenceEngine()\n        await mock_engine.initialize()\n        return await mock_engine.generate_completion(code, language, num_suggestions=num_suggestions)\n    \n    async def generate_chat_response(self, message: str, context: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Generate chat responses using real AI models\"\"\"\n        \n        if not self.is_initialized:\n            return await self._fallback_mock_chat(message, context)\n        \n        try:\n            start_time = time.time()\n            \n            if self.model_type == \"openai\":\n                response = await self._openai_chat(message, context)\n            elif self.model_type == \"ollama\":\n                response = await self._ollama_chat(message, context)\n            elif self.model_type == \"huggingface\":\n                response = await self._huggingface_chat(message, context)\n            else:\n                response = await self._fallback_mock_chat(message, context)\n            \n            processing_time = time.time() - start_time\n            response[\"processing_time_ms\"] = round(processing_time * 1000, 2)\n            response[\"model_used\"] = f\"{self.model_type}:{self.model_name}\"\n            \n            return response\n            \n        except Exception as e:\n            logger.error(f\"Real chat failed: {e}\")\n            return await self._fallback_mock_chat(message, context)\n    \n    async def _openai_chat(self, message: str, context: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Generate chat response using OpenAI\"\"\"\n        \n        system_prompt = \"\"\"You are CopilotMini, an expert programming assistant. Help users with:\n- Code completion and suggestions\n- Debugging and error fixing\n- Code explanation and documentation\n- Best practices and optimization\n- Architecture and design patterns\n\nBe concise, practical, and provide code examples when helpful.\"\"\"\n\n        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n        \n        if context:\n            messages.append({\"role\": \"system\", \"content\": f\"Code context:\\n```\\n{context}\\n```\"})\n        \n        messages.append({\"role\": \"user\", \"content\": message})\n        \n        response = await self.client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=messages,\n            max_tokens=500,\n            temperature=0.7\n        )\n        \n        return {\n            \"response\": response.choices[0].message.content,\n            \"confidence\": 0.90,\n            \"source\": \"gpt-4\"\n        }\n    \n    async def _ollama_chat(self, message: str, context: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Generate chat response using Ollama\"\"\"\n        import aiohttp\n        \n        prompt = f\"You are a helpful programming assistant.\\n\\nUser: {message}\\nAssistant:\"\n        if context:\n            prompt = f\"Code context:\\n{context}\\n\\n{prompt}\"\n        \n        payload = {\n            \"model\": self.model_name,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": {\"temperature\": 0.7, \"max_tokens\": 300}\n        }\n        \n        async with aiohttp.ClientSession() as session:\n            async with session.post(\"http://localhost:11434/api/generate\", json=payload) as response:\n                if response.status == 200:\n                    result = await response.json()\n                    return {\n                        \"response\": result.get(\"response\", \"Sorry, I couldn't generate a response.\"),\n                        \"confidence\": 0.80,\n                        \"source\": self.model_name\n                    }\n        \n        return {\"response\": \"Ollama service unavailable\", \"confidence\": 0.0, \"source\": \"error\"}\n    \n    async def _huggingface_chat(self, message: str, context: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Generate chat response using HuggingFace models\"\"\"\n        # For now, return a simple response since DialoGPT is not ideal for code\n        return {\n            \"response\": f\"I'm a basic HuggingFace model. For the question '{message}', I'd recommend checking the documentation or using a more advanced model.\",\n            \"confidence\": 0.60,\n            \"source\": self.model_name\n        }\n    \n    async def _fallback_mock_chat(self, message: str, context: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Fallback to mock chat if real AI fails\"\"\"\n        from .inference import MockInferenceEngine\n        \n        mock_engine = MockInferenceEngine()\n        await mock_engine.initialize()\n        return await mock_engine.generate_chat_response(message, context)\n\n    def get_status(self) -> str:\n        \"\"\"Get the current status of the inference engine.\"\"\"\n        if self.is_initialized:\n            return \"ready\"\n        else:\n            return \"initializing\"\n    \n    def get_model_info(self) -> Dict[str, Any]:\n        \"\"\"Get information about the loaded model.\"\"\"\n        return {\n            \"model_name\": self.model_name,\n            \"model_type\": self.model_type,\n            \"status\": self.get_status(),\n            \"capabilities\": [\"code_completion\", \"chat\", \"explanation\"],\n            \"supported_languages\": [\"python\", \"javascript\", \"java\", \"typescript\", \"go\", \"rust\", \"c\", \"cpp\", \"php\", \"ruby\"]\n        }\n    \n    async def cleanup(self):\n        \"\"\"Cleanup the inference engine.\"\"\"\n        logger.info(\"Cleaning up real inference engine...\")\n        self.is_initialized = False\n    \n    async def generate_completions(\n        self,\n        code: str,\n        language: str,\n        max_suggestions: int = 3,\n        cursor_position: Optional[int] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Generate code completions for the given input (compatible with mock engine).\"\"\"\n        \n        # Call the generate_completion method and adapt the response\n        suggestions_data = await self.generate_completion(code, language, num_suggestions=max_suggestions)\n        \n        # Extract just the text suggestions and confidence scores\n        suggestions = [s[\"text\"] for s in suggestions_data]\n        confidence_scores = [s[\"confidence\"] for s in suggestions_data]\n        \n        return {\n            \"suggestions\": suggestions,\n            \"confidence_scores\": confidence_scores,\n            \"model_used\": f\"{self.model_type}:{self.model_name}\",\n            \"language\": language,\n            \"processing_time\": suggestions_data[0].get(\"processing_time_ms\", 100) if suggestions_data else 100\n        }\n    \n    async def process_chat(\n        self,\n        message: str,\n        code_context: Optional[str] = None,\n        language: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Process a chat message and generate a response (compatible with mock engine).\"\"\"\n        \n        # Call the generate_chat_response method and adapt the response\n        response_data = await self.generate_chat_response(message, code_context)\n        \n        return {\n            \"response\": response_data[\"response\"],\n            \"code_suggestions\": None,  # Could be enhanced later\n            \"model_used\": f\"{self.model_type}:{self.model_name}\",\n            \"processing_time\": response_data.get(\"processing_time_ms\", 100)\n        }", "    def __init__(self):\n        self.client = None\n        self.model_type = \"openai\"  # or \"ollama\", \"huggingface\"\n        self.model_name = \"gpt-4\"\n        self.is_initialized = False", "    async def initialize(self) -> bool:\n        \"\"\"Initialize the real AI inference engine\"\"\"\n        try:\n            # Try OpenAI first\n            if await self._try_openai():\n                self.model_type = \"openai\"\n                logger.info(\"\u2705 OpenAI GPT-4 initialized successfully\")\n                self.is_initialized = True\n                return True\n            \n            # Fallback to Ollama\n            if await self._try_ollama():\n                self.model_type = \"ollama\"\n                logger.info(\"\u2705 Ollama local model initialized successfully\")\n                self.is_initialized = True\n                return True\n            \n            # Fallback to HuggingFace\n            if await self._try_huggingface():\n                self.model_type = \"huggingface\"\n                logger.info(\"\u2705 HuggingFace model initialized successfully\")\n                self.is_initialized = True\n                return True\n            \n            logger.warning(\"\u26a0\ufe0f No real AI models available, falling back to mock\")\n            return False\n            \n        except Exception as e:\n            logger.error(f\"Failed to initialize real inference engine: {e}\")\n            return False", "    async def _try_openai(self) -> bool:\n        \"\"\"Try to initialize OpenAI\"\"\"\n        try:\n            api_key = os.getenv(\"OPENAI_API_KEY\")\n            if not api_key:\n                logger.info(\"No OPENAI_API_KEY found, skipping OpenAI\")\n                return False\n            \n            import openai\n            self.client = openai.AsyncOpenAI(api_key=api_key)\n            \n            # Test the connection\n            response = await self.client.chat.completions.create(\n                model=\"gpt-4\",\n                messages=[{\"role\": \"user\", \"content\": \"test\"}],\n                max_tokens=5\n            )\n            return True\n            \n        except ImportError:\n            logger.info(\"OpenAI package not installed: pip install openai\")\n            return False\n        except Exception as e:\n            logger.info(f\"OpenAI initialization failed: {e}\")\n            return False", "    async def _try_ollama(self) -> bool:\n        \"\"\"Try to initialize Ollama local models\"\"\"\n        try:\n            import aiohttp\n            \n            # Check if Ollama is running\n            async with aiohttp.ClientSession() as session:\n                async with session.get(\"http://localhost:11434/api/tags\") as response:\n                    if response.status == 200:\n                        models = await response.json()\n                        available_models = [m[\"name\"] for m in models.get(\"models\", [])]\n                        \n                        # Prefer code-specific models\n                        preferred_models = [\"codellama:7b\", \"starcoder:3b\", \"codellama:13b\", \"llama2:7b\"]\n                        for model in preferred_models:\n                            if model in available_models:\n                                self.model_name = model\n                                logger.info(f\"Using Ollama model: {model}\")\n                                return True\n                        \n                        if available_models:\n                            self.model_name = available_models[0]\n                            logger.info(f\"Using Ollama model: {self.model_name}\")\n                            return True\n            \n            return False\n            \n        except ImportError:\n            logger.info(\"aiohttp package needed for Ollama: pip install aiohttp\")\n            return False\n        except Exception as e:\n            logger.info(f\"Ollama not available: {e}\")\n            return False", "    async def _try_huggingface(self) -> bool:\n        \"\"\"Try to initialize HuggingFace models\"\"\"\n        try:\n            from transformers import AutoModelForCausalLM, AutoTokenizer\n            import torch\n            \n            # Use a small, fast model for MVP\n            model_name = \"microsoft/DialoGPT-small\"  # Fast model for testing\n            \n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n            self.model = AutoModelForCausalLM.from_pretrained(model_name)\n            self.model_name = model_name\n            \n            # Add padding token if missing\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            logger.info(f\"Loaded HuggingFace model: {model_name}\")\n            return True\n            \n        except ImportError:\n            logger.info(\"HuggingFace transformers not installed: pip install transformers torch\")\n            return False\n        except Exception as e:\n            logger.info(f\"HuggingFace model loading failed: {e}\")\n            return False", "    async def generate_completion(\n        self,\n        code: str,\n        language: str,\n        max_length: int = 100,\n        num_suggestions: int = 3\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Generate code completions using real AI models\"\"\"\n        \n        if not self.is_initialized:\n            return await self._fallback_mock_completion(code, language, num_suggestions)\n        \n        try:\n            start_time = time.time()\n            \n            if self.model_type == \"openai\":\n                suggestions = await self._openai_completion(code, language, num_suggestions)\n            elif self.model_type == \"ollama\":\n                suggestions = await self._ollama_completion(code, language, num_suggestions)\n            elif self.model_type == \"huggingface\":\n                suggestions = await self._huggingface_completion(code, language, num_suggestions)\n            else:\n                suggestions = await self._fallback_mock_completion(code, language, num_suggestions)\n            \n            processing_time = time.time() - start_time\n            \n            # Add metadata to suggestions\n            for suggestion in suggestions:\n                suggestion[\"processing_time_ms\"] = round(processing_time * 1000, 2)\n                suggestion[\"model_used\"] = f\"{self.model_type}:{self.model_name}\"\n            \n            return suggestions[:num_suggestions]\n            \n        except Exception as e:\n            logger.error(f\"Real inference failed: {e}\")\n            return await self._fallback_mock_completion(code, language, num_suggestions)", "    async def _openai_completion(self, code: str, language: str, num_suggestions: int) -> List[Dict[str, Any]]:\n        \"\"\"Generate completions using OpenAI GPT-4. The 'code' argument may already include RAG context prepended.\"\"\"\n        \n        # Create a focused prompt for code completion\n        prompt = f\"\"\"You are an expert {language} programmer. Complete the following code with {num_suggestions} different suggestions.\nOnly return the completion code without explanations.\n\nCode to complete:\n```{language}\n{code}\n```\n\nComplete this code with the most likely continuation:\"\"\"\n\n        response = await self.client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a code completion AI. Only return code completions, no explanations.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            max_tokens=150,\n            temperature=0.3,\n            n=num_suggestions\n        )\n        \n        suggestions = []\n        for i, choice in enumerate(response.choices):\n            completion = choice.message.content.strip()\n            # Clean up the completion\n            if \"```\" in completion:\n                completion = completion.split(\"```\")[1].strip()\n                if completion.startswith(language):\n                    completion = completion[len(language):].strip()\n            \n            suggestions.append({\n                \"text\": completion,\n                \"confidence\": 0.85 + (i * 0.05),  # Decreasing confidence\n                \"source\": \"gpt-4\"\n            })\n        \n        return suggestions", "    async def _ollama_completion(self, code: str, language: str, num_suggestions: int) -> List[Dict[str, Any]]:\n        \"\"\"Generate completions using Ollama local models. The 'code' argument may already include RAG context prepended.\"\"\"\n        import aiohttp\n        \n        prompt = f\"Complete this {language} code:\\n{code}\"\n        \n        suggestions = []\n        async with aiohttp.ClientSession() as session:\n            for i in range(num_suggestions):\n                payload = {\n                    \"model\": self.model_name,\n                    \"prompt\": prompt,\n                    \"stream\": False,\n                    \"options\": {\n                        \"temperature\": 0.3 + (i * 0.2),  # Vary temperature for diversity\n                        \"top_p\": 0.9,\n                        \"max_tokens\": 100\n                    }\n                }\n                \n                async with session.post(\"http://localhost:11434/api/generate\", json=payload) as response:\n                    if response.status == 200:\n                        result = await response.json()\n                        completion = result.get(\"response\", \"\").strip()\n                        \n                        suggestions.append({\n                            \"text\": completion,\n                            \"confidence\": 0.80 - (i * 0.1),\n                            \"source\": self.model_name\n                        })\n        \n        return suggestions", "    async def _huggingface_completion(self, code: str, language: str, num_suggestions: int) -> List[Dict[str, Any]]:\n        \"\"\"Generate completions using HuggingFace models. The 'code' argument may already include RAG context prepended.\"\"\"\n        import torch\n        \n        # Prepare input\n        prompt = f\"# {language} code completion\\n{code}\"\n        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n        \n        suggestions = []\n        for i in range(num_suggestions):\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    inputs,\n                    max_length=inputs.shape[1] + 50,\n                    temperature=0.7 + (i * 0.2),\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.eos_token_id\n                )\n            \n            # Decode completion\n            completion = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n            \n            suggestions.append({\n                \"text\": completion.strip(),\n                \"confidence\": 0.75 - (i * 0.1),\n                \"source\": self.model_name\n            })\n        \n        return suggestions", "    async def _fallback_mock_completion(self, code: str, language: str, num_suggestions: int) -> List[Dict[str, Any]]:\n        \"\"\"Fallback to mock completions if real AI fails\"\"\"\n        from .inference import MockInferenceEngine\n        \n        mock_engine = MockInferenceEngine()\n        await mock_engine.initialize()\n        return await mock_engine.generate_completion(code, language, num_suggestions=num_suggestions)", "    async def generate_chat_response(self, message: str, context: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Generate chat responses using real AI models\"\"\"\n        \n        if not self.is_initialized:\n            return await self._fallback_mock_chat(message, context)\n        \n        try:\n            start_time = time.time()\n            \n            if self.model_type == \"openai\":\n                response = await self._openai_chat(message, context)\n            elif self.model_type == \"ollama\":\n                response = await self._ollama_chat(message, context)\n            elif self.model_type == \"huggingface\":\n                response = await self._huggingface_chat(message, context)\n            else:\n                response = await self._fallback_mock_chat(message, context)\n            \n            processing_time = time.time() - start_time\n            response[\"processing_time_ms\"] = round(processing_time * 1000, 2)\n            response[\"model_used\"] = f\"{self.model_type}:{self.model_name}\"\n            \n            return response\n            \n        except Exception as e:\n            logger.error(f\"Real chat failed: {e}\")\n            return await self._fallback_mock_chat(message, context)", "    async def _openai_chat(self, message: str, context: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Generate chat response using OpenAI\"\"\"\n        \n        system_prompt = \"\"\"You are CopilotMini, an expert programming assistant. Help users with:\n- Code completion and suggestions\n- Debugging and error fixing\n- Code explanation and documentation\n- Best practices and optimization\n- Architecture and design patterns\n\nBe concise, practical, and provide code examples when helpful.\"\"\"\n\n        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n        \n        if context:\n            messages.append({\"role\": \"system\", \"content\": f\"Code context:\\n```\\n{context}\\n```\"})\n        \n        messages.append({\"role\": \"user\", \"content\": message})\n        \n        response = await self.client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=messages,\n            max_tokens=500,\n            temperature=0.7\n        )\n        \n        return {\n            \"response\": response.choices[0].message.content,\n            \"confidence\": 0.90,\n            \"source\": \"gpt-4\"\n        }", "    async def _ollama_chat(self, message: str, context: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Generate chat response using Ollama\"\"\"\n        import aiohttp\n        \n        prompt = f\"You are a helpful programming assistant.\\n\\nUser: {message}\\nAssistant:\"\n        if context:\n            prompt = f\"Code context:\\n{context}\\n\\n{prompt}\"\n        \n        payload = {\n            \"model\": self.model_name,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": {\"temperature\": 0.7, \"max_tokens\": 300}\n        }\n        \n        async with aiohttp.ClientSession() as session:\n            async with session.post(\"http://localhost:11434/api/generate\", json=payload) as response:\n                if response.status == 200:\n                    result = await response.json()\n                    return {\n                        \"response\": result.get(\"response\", \"Sorry, I couldn't generate a response.\"),\n                        \"confidence\": 0.80,\n                        \"source\": self.model_name\n                    }\n        \n        return {\"response\": \"Ollama service unavailable\", \"confidence\": 0.0, \"source\": \"error\"}", "    async def _huggingface_chat(self, message: str, context: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Generate chat response using HuggingFace models\"\"\"\n        # For now, return a simple response since DialoGPT is not ideal for code\n        return {\n            \"response\": f\"I'm a basic HuggingFace model. For the question '{message}', I'd recommend checking the documentation or using a more advanced model.\",\n            \"confidence\": 0.60,\n            \"source\": self.model_name\n        }", "    async def _fallback_mock_chat(self, message: str, context: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Fallback to mock chat if real AI fails\"\"\"\n        from .inference import MockInferenceEngine\n        \n        mock_engine = MockInferenceEngine()\n        await mock_engine.initialize()\n        return await mock_engine.generate_chat_response(message, context)", "    def get_status(self) -> str:\n        \"\"\"Get the current status of the inference engine.\"\"\"\n        if self.is_initialized:\n            return \"ready\"\n        else:\n            return \"initializing\"", "    def get_model_info(self) -> Dict[str, Any]:\n        \"\"\"Get information about the loaded model.\"\"\"\n        return {\n            \"model_name\": self.model_name,\n            \"model_type\": self.model_type,\n            \"status\": self.get_status(),\n            \"capabilities\": [\"code_completion\", \"chat\", \"explanation\"],\n            \"supported_languages\": [\"python\", \"javascript\", \"java\", \"typescript\", \"go\", \"rust\", \"c\", \"cpp\", \"php\", \"ruby\"]\n        }", "    async def cleanup(self):\n        \"\"\"Cleanup the inference engine.\"\"\"\n        logger.info(\"Cleaning up real inference engine...\")\n        self.is_initialized = False", "    async def generate_completions(\n        self,\n        code: str,\n        language: str,\n        max_suggestions: int = 3,\n        cursor_position: Optional[int] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Generate code completions for the given input (compatible with mock engine).\"\"\"\n        \n        # Call the generate_completion method and adapt the response\n        suggestions_data = await self.generate_completion(code, language, num_suggestions=max_suggestions)\n        \n        # Extract just the text suggestions and confidence scores\n        suggestions = [s[\"text\"] for s in suggestions_data]\n        confidence_scores = [s[\"confidence\"] for s in suggestions_data]\n        \n        return {\n            \"suggestions\": suggestions,\n            \"confidence_scores\": confidence_scores,\n            \"model_used\": f\"{self.model_type}:{self.model_name}\",\n            \"language\": language,\n            \"processing_time\": suggestions_data[0].get(\"processing_time_ms\", 100) if suggestions_data else 100\n        }", "    async def process_chat(\n        self,\n        message: str,\n        code_context: Optional[str] = None,\n        language: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Process a chat message and generate a response (compatible with mock engine).\"\"\"\n        \n        # Call the generate_chat_response method and adapt the response\n        response_data = await self.generate_chat_response(message, code_context)\n        \n        return {\n            \"response\": response_data[\"response\"],\n            \"code_suggestions\": None,  # Could be enhanced later\n            \"model_used\": f\"{self.model_type}:{self.model_name}\",\n            \"processing_time\": response_data.get(\"processing_time_ms\", 100)\n        }", "# Backend models and inference engines ", "class MockInferenceEngine:\n    \"\"\"Mock inference engine that simulates code completion and chat responses.\"\"\"\n    \n    def __init__(self):\n        self.status = \"initializing\"\n        self.model_name = \"mock-codeparrot-v1\"\n        self.initialized = False\n        \n        # Mock completions database\n        self.mock_completions = {\n            \"python\": [\n                \"print('Hello, World!')\",\n                \"def __init__(self):\",\n                \"if __name__ == '__main__':\",\n                \"return result\",\n                \"except Exception as e:\",\n                \"for i in range(len(items)):\",\n                \"with open('file.txt', 'r') as f:\",\n                \"import numpy as np\",\n                \"class MyClass:\",\n                \"async def process_data():\"\n            ],\n            \"javascript\": [\n                \"console.log('Hello, World!');\",\n                \"function() {\",\n                \"const result = \",\n                \"return new Promise(\",\n                \"} catch (error) {\",\n                \"for (let i = 0; i < array.length; i++) {\",\n                \"addEventListener('click', () => {\",\n                \"import React from 'react';\",\n                \"export default class\",\n                \"async function fetchData()\"\n            ],\n            \"java\": [\n                \"System.out.println(\\\"Hello, World!\\\");\",\n                \"public class MyClass {\",\n                \"public static void main(String[] args) {\",\n                \"private static final\",\n                \"} catch (Exception e) {\",\n                \"for (int i = 0; i < array.length; i++) {\",\n                \"new ArrayList<>()\",\n                \"import java.util.*;\",\n                \"public void method() {\",\n                \"@Override\"\n            ],\n            \"default\": [\n                \"// TODO: Implement this\",\n                \"/* Comment */\",\n                \"function main() {\",\n                \"return 0;\",\n                \"end\",\n                \"class Example\",\n                \"if condition:\",\n                \"else:\",\n                \"while loop:\",\n                \"break;\"\n            ]\n        }\n        \n        # Mock chat responses\n        self.chat_responses = [\n            \"This code looks good! Here's what it does:\",\n            \"I can help you with that. Consider this approach:\",\n            \"There's a potential issue here. You might want to:\",\n            \"Great question! The best practice would be to:\",\n            \"I see what you're trying to do. A more efficient way would be:\",\n            \"This pattern is commonly used for:\",\n            \"You could optimize this by:\",\n            \"Here's how you can fix this error:\",\n            \"Consider using this alternative approach:\",\n            \"This code follows good practices, but you could also:\"\n        ]\n    \n    async def initialize(self):\n        \"\"\"Initialize the mock inference engine.\"\"\"\n        logger.info(\"Initializing mock inference engine...\")\n        \n        # Simulate initialization time\n        await asyncio.sleep(0.5)\n        \n        self.status = \"ready\"\n        self.initialized = True\n        \n        logger.info(\"Mock inference engine initialized successfully\")\n    \n    async def cleanup(self):\n        \"\"\"Cleanup the inference engine.\"\"\"\n        logger.info(\"Cleaning up mock inference engine...\")\n        self.status = \"shutdown\"\n        self.initialized = False\n    \n    def get_status(self) -> str:\n        \"\"\"Get the current status of the inference engine.\"\"\"\n        return self.status\n    \n    def get_model_info(self) -> Dict[str, Any]:\n        \"\"\"Get information about the loaded model.\"\"\"\n        return {\n            \"model_name\": self.model_name,\n            \"model_type\": \"mock\",\n            \"status\": self.status,\n            \"capabilities\": [\"code_completion\", \"chat\", \"explanation\"],\n            \"supported_languages\": [\"python\", \"javascript\", \"java\", \"typescript\", \"go\", \"rust\"]\n        }\n    \n    async def generate_completions(\n        self,\n        code: str,\n        language: str,\n        max_suggestions: int = 3,\n        cursor_position: Optional[int] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Generate code completions for the given input.\"\"\"\n        if not self.initialized:\n            raise RuntimeError(\"Inference engine not initialized\")\n        \n        # Simulate processing time\n        await asyncio.sleep(random.uniform(0.1, 0.3))\n        \n        # Get language-specific completions or default\n        available_completions = self.mock_completions.get(language.lower(), self.mock_completions[\"default\"])\n        \n        # Generate suggestions based on context\n        suggestions = self._generate_contextual_suggestions(code, available_completions, max_suggestions)\n        \n        # Generate mock confidence scores\n        confidence_scores = [random.uniform(0.7, 0.95) for _ in suggestions]\n        confidence_scores.sort(reverse=True)  # Higher confidence first\n        \n        return {\n            \"suggestions\": suggestions,\n            \"confidence_scores\": confidence_scores,\n            \"model_used\": self.model_name,\n            \"language\": language,\n            \"processing_time\": random.uniform(50, 200)  # Mock processing time in ms\n        }\n    \n    def _generate_contextual_suggestions(self, code: str, available_completions: List[str], max_suggestions: int) -> List[str]:\n        \"\"\"Generate contextual suggestions based on the input code.\"\"\"\n        suggestions = []\n        \n        # Analyze the code context for better suggestions\n        code_lower = code.lower().strip()\n        last_line = code.split('\\n')[-1] if code else \"\"\n        \n        # Context-aware suggestion logic\n        if \"def \" in last_line or \"function \" in last_line:\n            # Suggest function body starters\n            function_suggestions = [comp for comp in available_completions if any(keyword in comp for keyword in [\"return\", \"if\", \"for\", \"try\"])]\n            suggestions.extend(random.sample(function_suggestions, min(len(function_suggestions), max_suggestions)))\n        \n        elif \"class \" in last_line:\n            # Suggest class body starters\n            class_suggestions = [comp for comp in available_completions if any(keyword in comp for keyword in [\"def\", \"__init__\", \"self\"])]\n            suggestions.extend(random.sample(class_suggestions, min(len(class_suggestions), max_suggestions)))\n        \n        elif \"import \" in last_line or \"from \" in last_line:\n            # Suggest more imports\n            import_suggestions = [comp for comp in available_completions if \"import\" in comp]\n            suggestions.extend(random.sample(import_suggestions, min(len(import_suggestions), max_suggestions)))\n        \n        elif \"print\" in last_line or \"console.log\" in last_line:\n            # Suggest similar output statements\n            output_suggestions = [comp for comp in available_completions if any(keyword in comp for keyword in [\"print\", \"console.log\", \"System.out\"])]\n            suggestions.extend(random.sample(output_suggestions, min(len(output_suggestions), max_suggestions)))\n        \n        else:\n            # Default random suggestions\n            suggestions.extend(random.sample(available_completions, min(len(available_completions), max_suggestions)))\n        \n        # Remove duplicates and limit to max_suggestions\n        suggestions = list(dict.fromkeys(suggestions))[:max_suggestions]\n        \n        # If we don't have enough suggestions, fill with random ones\n        while len(suggestions) < max_suggestions and len(suggestions) < len(available_completions):\n            remaining = [comp for comp in available_completions if comp not in suggestions]\n            if remaining:\n                suggestions.append(random.choice(remaining))\n        \n        return suggestions\n    \n    async def process_chat(\n        self,\n        message: str,\n        code_context: Optional[str] = None,\n        language: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Process a chat message and generate a response.\"\"\"\n        if not self.initialized:\n            raise RuntimeError(\"Inference engine not initialized\")\n        \n        # Simulate processing time\n        await asyncio.sleep(random.uniform(0.2, 0.5))\n        \n        # Generate contextual response\n        response = self._generate_chat_response(message, code_context, language)\n        \n        # Sometimes include code suggestions\n        code_suggestions = None\n        if random.random() < 0.3 and language:  # 30% chance to include code suggestions\n            completions = await self.generate_completions(\"\", language, max_suggestions=2)\n            code_suggestions = completions[\"suggestions\"]\n        \n        return {\n            \"response\": response,\n            \"code_suggestions\": code_suggestions,\n            \"model_used\": self.model_name,\n            \"context_used\": bool(code_context),\n            \"language\": language\n        }\n    \n    def _generate_chat_response(self, message: str, code_context: Optional[str], language: Optional[str]) -> str:\n        \"\"\"Generate a contextual chat response.\"\"\"\n        message_lower = message.lower()\n        \n        # Context-aware responses\n        if any(keyword in message_lower for keyword in [\"error\", \"bug\", \"fix\", \"wrong\"]):\n            responses = [\n                \"I can help you debug this issue. Let me analyze the code...\",\n                \"This looks like a common error. Here's what might be causing it:\",\n                \"I see the problem! The issue is likely in this section:\",\n                \"Let's fix this step by step. First, check if:\",\n            ]\n        elif any(keyword in message_lower for keyword in [\"explain\", \"what\", \"how\", \"why\"]):\n            responses = [\n                \"Let me explain how this works:\",\n                \"This code does the following:\",\n                \"The purpose of this function is to:\",\n                \"Here's a breakdown of what's happening:\",\n            ]\n        elif any(keyword in message_lower for keyword in [\"optimize\", \"improve\", \"better\", \"performance\"]):\n            responses = [\n                \"Here are some ways to optimize this code:\",\n                \"You can improve performance by:\",\n                \"Consider these optimizations:\",\n                \"A more efficient approach would be:\",\n            ]\n        elif any(keyword in message_lower for keyword in [\"refactor\", \"clean\", \"structure\"]):\n            responses = [\n                \"Here's how you could refactor this code:\",\n                \"To improve code structure, consider:\",\n                \"A cleaner approach would be:\",\n                \"You could reorganize this by:\",\n            ]\n        else:\n            responses = self.chat_responses\n        \n        base_response = random.choice(responses)\n        \n        # Add context-specific details\n        if code_context:\n            base_response += f\" Looking at your {language or 'code'}, I notice that...\"\n        elif language:\n            base_response += f\" In {language}, the best practice is...\"\n        \n        return base_response\n    \n    async def explain_code(self, code: str, language: str) -> Dict[str, Any]:\n        \"\"\"Explain what a piece of code does.\"\"\"\n        if not self.initialized:\n            raise RuntimeError(\"Inference engine not initialized\")\n        \n        await asyncio.sleep(random.uniform(0.1, 0.3))\n        \n        # Generate explanation based on code patterns\n        explanation = self._generate_code_explanation(code, language)\n        \n        return {\n            \"explanation\": explanation,\n            \"language\": language,\n            \"model_used\": self.model_name,\n            \"confidence\": random.uniform(0.8, 0.95)\n        }\n    \n    def _generate_code_explanation(self, code: str, language: str) -> str:\n        \"\"\"Generate an explanation of the code.\"\"\"\n        explanations = [\n            f\"This {language} code defines a function that processes data.\",\n            f\"This is a {language} class implementation with multiple methods.\",\n            f\"This {language} code demonstrates error handling and input validation.\",\n            f\"This is a {language} utility function for data manipulation.\",\n            f\"This {language} code implements a common design pattern.\",\n            f\"This is a {language} function that performs calculations and returns results.\",\n            f\"This {language} code handles file operations and data processing.\",\n            f\"This is a {language} implementation of an algorithm or data structure.\"\n        ]\n        \n        return random.choice(explanations) ", "    def __init__(self):\n        self.status = \"initializing\"\n        self.model_name = \"mock-codeparrot-v1\"\n        self.initialized = False\n        \n        # Mock completions database\n        self.mock_completions = {\n            \"python\": [\n                \"print('Hello, World!')\",\n                \"def __init__(self):\",\n                \"if __name__ == '__main__':\",\n                \"return result\",\n                \"except Exception as e:\",\n                \"for i in range(len(items)):\",\n                \"with open('file.txt', 'r') as f:\",\n                \"import numpy as np\",\n                \"class MyClass:\",\n                \"async def process_data():\"\n            ],\n            \"javascript\": [\n                \"console.log('Hello, World!');\",\n                \"function() {\",\n                \"const result = \",\n                \"return new Promise(\",\n                \"} catch (error) {\",\n                \"for (let i = 0; i < array.length; i++) {\",\n                \"addEventListener('click', () => {\",\n                \"import React from 'react';\",\n                \"export default class\",\n                \"async function fetchData()\"\n            ],\n            \"java\": [\n                \"System.out.println(\\\"Hello, World!\\\");\",\n                \"public class MyClass {\",\n                \"public static void main(String[] args) {\",\n                \"private static final\",\n                \"} catch (Exception e) {\",\n                \"for (int i = 0; i < array.length; i++) {\",\n                \"new ArrayList<>()\",\n                \"import java.util.*;\",\n                \"public void method() {\",\n                \"@Override\"\n            ],\n            \"default\": [\n                \"// TODO: Implement this\",\n                \"/* Comment */\",\n                \"function main() {\",\n                \"return 0;\",\n                \"end\",\n                \"class Example\",\n                \"if condition:\",\n                \"else:\",\n                \"while loop:\",\n                \"break;\"\n            ]\n        }\n        \n        # Mock chat responses\n        self.chat_responses = [\n            \"This code looks good! Here's what it does:\",\n            \"I can help you with that. Consider this approach:\",\n            \"There's a potential issue here. You might want to:\",\n            \"Great question! The best practice would be to:\",\n            \"I see what you're trying to do. A more efficient way would be:\",\n            \"This pattern is commonly used for:\",\n            \"You could optimize this by:\",\n            \"Here's how you can fix this error:\",\n            \"Consider using this alternative approach:\",\n            \"This code follows good practices, but you could also:\"\n        ]", "    async def initialize(self):\n        \"\"\"Initialize the mock inference engine.\"\"\"\n        logger.info(\"Initializing mock inference engine...\")\n        \n        # Simulate initialization time\n        await asyncio.sleep(0.5)\n        \n        self.status = \"ready\"\n        self.initialized = True\n        \n        logger.info(\"Mock inference engine initialized successfully\")", "    async def cleanup(self):\n        \"\"\"Cleanup the inference engine.\"\"\"\n        logger.info(\"Cleaning up mock inference engine...\")\n        self.status = \"shutdown\"\n        self.initialized = False", "    def get_status(self) -> str:\n        \"\"\"Get the current status of the inference engine.\"\"\"\n        return self.status", "    def get_model_info(self) -> Dict[str, Any]:\n        \"\"\"Get information about the loaded model.\"\"\"\n        return {\n            \"model_name\": self.model_name,\n            \"model_type\": \"mock\",\n            \"status\": self.status,\n            \"capabilities\": [\"code_completion\", \"chat\", \"explanation\"],\n            \"supported_languages\": [\"python\", \"javascript\", \"java\", \"typescript\", \"go\", \"rust\"]\n        }", "    async def generate_completions(\n        self,\n        code: str,\n        language: str,\n        max_suggestions: int = 3,\n        cursor_position: Optional[int] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Generate code completions for the given input.\"\"\"\n        if not self.initialized:\n            raise RuntimeError(\"Inference engine not initialized\")\n        \n        # Simulate processing time\n        await asyncio.sleep(random.uniform(0.1, 0.3))\n        \n        # Get language-specific completions or default\n        available_completions = self.mock_completions.get(language.lower(), self.mock_completions[\"default\"])\n        \n        # Generate suggestions based on context\n        suggestions = self._generate_contextual_suggestions(code, available_completions, max_suggestions)\n        \n        # Generate mock confidence scores\n        confidence_scores = [random.uniform(0.7, 0.95) for _ in suggestions]\n        confidence_scores.sort(reverse=True)  # Higher confidence first\n        \n        return {\n            \"suggestions\": suggestions,\n            \"confidence_scores\": confidence_scores,\n            \"model_used\": self.model_name,\n            \"language\": language,\n            \"processing_time\": random.uniform(50, 200)  # Mock processing time in ms\n        }", "    def _generate_contextual_suggestions(self, code: str, available_completions: List[str], max_suggestions: int) -> List[str]:\n        \"\"\"Generate contextual suggestions based on the input code.\"\"\"\n        suggestions = []\n        \n        # Analyze the code context for better suggestions\n        code_lower = code.lower().strip()\n        last_line = code.split('\\n')[-1] if code else \"\"\n        \n        # Context-aware suggestion logic\n        if \"def \" in last_line or \"function \" in last_line:\n            # Suggest function body starters\n            function_suggestions = [comp for comp in available_completions if any(keyword in comp for keyword in [\"return\", \"if\", \"for\", \"try\"])]\n            suggestions.extend(random.sample(function_suggestions, min(len(function_suggestions), max_suggestions)))\n        \n        elif \"class \" in last_line:\n            # Suggest class body starters\n            class_suggestions = [comp for comp in available_completions if any(keyword in comp for keyword in [\"def\", \"__init__\", \"self\"])]\n            suggestions.extend(random.sample(class_suggestions, min(len(class_suggestions), max_suggestions)))\n        \n        elif \"import \" in last_line or \"from \" in last_line:\n            # Suggest more imports\n            import_suggestions = [comp for comp in available_completions if \"import\" in comp]\n            suggestions.extend(random.sample(import_suggestions, min(len(import_suggestions), max_suggestions)))\n        \n        elif \"print\" in last_line or \"console.log\" in last_line:\n            # Suggest similar output statements\n            output_suggestions = [comp for comp in available_completions if any(keyword in comp for keyword in [\"print\", \"console.log\", \"System.out\"])]\n            suggestions.extend(random.sample(output_suggestions, min(len(output_suggestions), max_suggestions)))\n        \n        else:\n            # Default random suggestions\n            suggestions.extend(random.sample(available_completions, min(len(available_completions), max_suggestions)))\n        \n        # Remove duplicates and limit to max_suggestions\n        suggestions = list(dict.fromkeys(suggestions))[:max_suggestions]\n        \n        # If we don't have enough suggestions, fill with random ones\n        while len(suggestions) < max_suggestions and len(suggestions) < len(available_completions):\n            remaining = [comp for comp in available_completions if comp not in suggestions]\n            if remaining:\n                suggestions.append(random.choice(remaining))\n        \n        return suggestions", "    async def process_chat(\n        self,\n        message: str,\n        code_context: Optional[str] = None,\n        language: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Process a chat message and generate a response.\"\"\"\n        if not self.initialized:\n            raise RuntimeError(\"Inference engine not initialized\")\n        \n        # Simulate processing time\n        await asyncio.sleep(random.uniform(0.2, 0.5))\n        \n        # Generate contextual response\n        response = self._generate_chat_response(message, code_context, language)\n        \n        # Sometimes include code suggestions\n        code_suggestions = None\n        if random.random() < 0.3 and language:  # 30% chance to include code suggestions\n            completions = await self.generate_completions(\"\", language, max_suggestions=2)\n            code_suggestions = completions[\"suggestions\"]\n        \n        return {\n            \"response\": response,\n            \"code_suggestions\": code_suggestions,\n            \"model_used\": self.model_name,\n            \"context_used\": bool(code_context),\n            \"language\": language\n        }", "    def _generate_chat_response(self, message: str, code_context: Optional[str], language: Optional[str]) -> str:\n        \"\"\"Generate a contextual chat response.\"\"\"\n        message_lower = message.lower()\n        \n        # Context-aware responses\n        if any(keyword in message_lower for keyword in [\"error\", \"bug\", \"fix\", \"wrong\"]):\n            responses = [\n                \"I can help you debug this issue. Let me analyze the code...\",\n                \"This looks like a common error. Here's what might be causing it:\",\n                \"I see the problem! The issue is likely in this section:\",\n                \"Let's fix this step by step. First, check if:\",\n            ]\n        elif any(keyword in message_lower for keyword in [\"explain\", \"what\", \"how\", \"why\"]):\n            responses = [\n                \"Let me explain how this works:\",\n                \"This code does the following:\",\n                \"The purpose of this function is to:\",\n                \"Here's a breakdown of what's happening:\",\n            ]\n        elif any(keyword in message_lower for keyword in [\"optimize\", \"improve\", \"better\", \"performance\"]):\n            responses = [\n                \"Here are some ways to optimize this code:\",\n                \"You can improve performance by:\",\n                \"Consider these optimizations:\",\n                \"A more efficient approach would be:\",\n            ]\n        elif any(keyword in message_lower for keyword in [\"refactor\", \"clean\", \"structure\"]):\n            responses = [\n                \"Here's how you could refactor this code:\",\n                \"To improve code structure, consider:\",\n                \"A cleaner approach would be:\",\n                \"You could reorganize this by:\",\n            ]\n        else:\n            responses = self.chat_responses\n        \n        base_response = random.choice(responses)\n        \n        # Add context-specific details\n        if code_context:\n            base_response += f\" Looking at your {language or 'code'}, I notice that...\"\n        elif language:\n            base_response += f\" In {language}, the best practice is...\"\n        \n        return base_response", "    async def explain_code(self, code: str, language: str) -> Dict[str, Any]:\n        \"\"\"Explain what a piece of code does.\"\"\"\n        if not self.initialized:\n            raise RuntimeError(\"Inference engine not initialized\")\n        \n        await asyncio.sleep(random.uniform(0.1, 0.3))\n        \n        # Generate explanation based on code patterns\n        explanation = self._generate_code_explanation(code, language)\n        \n        return {\n            \"explanation\": explanation,\n            \"language\": language,\n            \"model_used\": self.model_name,\n            \"confidence\": random.uniform(0.8, 0.95)\n        }", "    def _generate_code_explanation(self, code: str, language: str) -> str:\n        \"\"\"Generate an explanation of the code.\"\"\"\n        explanations = [\n            f\"This {language} code defines a function that processes data.\",\n            f\"This is a {language} class implementation with multiple methods.\",\n            f\"This {language} code demonstrates error handling and input validation.\",\n            f\"This is a {language} utility function for data manipulation.\",\n            f\"This {language} code implements a common design pattern.\",\n            f\"This is a {language} function that performs calculations and returns results.\",\n            f\"This {language} code handles file operations and data processing.\",\n            f\"This is a {language} implementation of an algorithm or data structure.\"\n        ]\n        \n        return random.choice(explanations) ", "class CustomInferenceEngine:\n    \"\"\"Custom inference engine for the fine-tuned CodeGen model.\"\"\"\n    \n    def __init__(self):\n        self.models = {}\n        self.tokenizers = {}\n        self.model_configs = {}\n        self.default_model = None\n        self.is_initialized = False\n        \n        # Model type mappings\n        self.model_types = {\n            \"codegen\": {\n                \"model_class\": AutoModelForCausalLM,\n                \"tokenizer_class\": AutoTokenizer,\n                \"type\": \"causal_lm\"\n            }\n        }\n        \n    async def initialize(self) -> bool:\n        \"\"\"Initialize the custom inference engine by loading the CodeGen model.\"\"\"\n        logger.info(\"\ud83e\udd16 Initializing custom inference engine...\")\n        \n        try:\n            # Discover the trained CodeGen model\n            model_info = self._discover_trained_model()\n            \n            if not model_info:\n                logger.warning(\"\u26a0\ufe0f No trained CodeGen model found, cannot initialize custom inference\")\n                return False\n            \n            # Load the model\n            if await self._load_model(model_info):\n                self.default_model = \"codegen\"\n                self.is_initialized = True\n                logger.info(\"\u2705 Custom inference engine initialized with CodeGen model\")\n                logger.info(f\"Default model: {self.default_model}\")\n                return True\n            else:\n                logger.error(\"\u274c Failed to load the custom CodeGen model\")\n                return False\n                \n        except Exception as e:\n            logger.error(f\"Failed to initialize custom inference engine: {e}\")\n            return False\n    \n    def _discover_trained_model(self) -> Optional[Dict[str, Any]]:\n        \"\"\"Discover the available trained CodeGen model.\"\"\"\n        checkpoints_dir = Path(\"training/checkpoints\")\n        model_type = \"codegen\"\n        \n        model_path = checkpoints_dir / model_type\n        \n        if model_path.exists() and any(model_path.iterdir()):\n            logger.info(f\"Found trained model: {model_type} at {model_path}\")\n            return {\n                \"model_type\": model_type,\n                \"path\": str(model_path)\n            }\n        return None\n    \n    async def _load_model(self, model_info: Dict[str, Any]) -> bool:\n        \"\"\"Load a specific trained model.\"\"\"\n        model_type = model_info[\"model_type\"]\n        model_path = model_info[\"path\"]\n        \n        logger.info(f\"Loading {model_type} model from {model_path}\")\n        \n        try:\n            # Get model configuration\n            model_config = self.model_types[model_type]\n            \n            # Load tokenizer\n            tokenizer_class = model_config[\"tokenizer_class\"]\n            tokenizer = tokenizer_class.from_pretrained(model_path)\n            \n            # Load model\n            model_class = model_config[\"model_class\"]\n            model = model_class.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                device_map=\"auto\" if torch.cuda.is_available() else None\n            )\n            \n            # Store model and tokenizer\n            self.models[model_type] = model\n            self.tokenizers[model_type] = tokenizer\n            self.model_configs[model_type] = model_config\n            \n            logger.info(f\"\u2705 Successfully loaded {model_type} model\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Failed to load {model_type} model: {e}\")\n            return False\n    \n    async def generate_completions(\n        self,\n        code: str,\n        language: str,\n        max_suggestions: int = 3,\n        cursor_position: Optional[int] = None,\n        model_type: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Generate code completions using the custom CodeGen model.\"\"\"\n        \n        if not self.is_initialized:\n            raise RuntimeError(\"Custom inference engine not initialized\")\n        \n        selected_model = \"codegen\"\n        \n        start_time = time.time()\n        \n        try:\n            model = self.models[selected_model]\n            tokenizer = self.tokenizers[selected_model]\n            \n            suggestions = await self._generate_causal_completions(\n                model, tokenizer, code, max_suggestions\n            )\n            \n            processing_time = (time.time() - start_time) * 1000  # Convert to ms\n            \n            # Calculate confidence scores\n            confidence_scores = [0.9 - (i * 0.1) for i in range(len(suggestions))]\n            \n            return {\n                \"suggestions\": suggestions,\n                \"confidence_scores\": confidence_scores,\n                \"model_used\": f\"custom:{selected_model}\",\n                \"language\": language,\n                \"processing_time\": processing_time\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error generating completions with {selected_model}: {e}\")\n            raise\n    \n    async def _generate_causal_completions(\n        self,\n        model,\n        tokenizer,\n        code: str,\n        max_suggestions: int\n    ) -> List[str]:\n        \"\"\"Generate completions using causal language models (CodeGen).\"\"\"\n        \n        model.eval()\n        \n        # Tokenize input\n        inputs = tokenizer(code, return_tensors=\"pt\")\n        \n        # Handle device placement for Apple Silicon\n        if torch.cuda.is_available():\n            inputs = {k: v.cuda() for k, v in inputs.items()}\n            model = model.cuda()\n        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n            inputs = {k: v.to('mps') for k, v in inputs.items()}\n            model = model.to('mps')\n        \n        suggestions = []\n        \n        try:\n            with torch.no_grad():\n                # Try sampling first with safer parameters\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=30,\n                    num_return_sequences=max_suggestions,\n                    do_sample=True,\n                    temperature=0.1,  # Lower temperature for stability\n                    top_p=0.9,        # Lower top_p for stability\n                    top_k=50,         # Add top_k for additional stability\n                    pad_token_id=tokenizer.eos_token_id,\n                    repetition_penalty=1.1,  # Prevent repetition\n                    no_repeat_ngram_size=2   # Prevent repetition\n                )\n                \n                # Decode and clean suggestions\n                suggestions = [\n                    tokenizer.decode(output, skip_special_tokens=True)\n                    for output in outputs\n                ]\n                \n                # Remove the original code from the suggestion\n                suggestions = [s[len(code):].strip() for s in suggestions]\n                \n        except Exception as e:\n            logger.warning(f\"Sampling generation failed: {e}, falling back to greedy decoding\")\n            \n            # Fallback to greedy decoding (more stable)\n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=30,\n                    num_return_sequences=1,\n                    do_sample=False,  # Greedy decoding\n                    pad_token_id=tokenizer.eos_token_id\n                )\n                \n                # Decode and create variations\n                base_suggestion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n                base_suggestion = base_suggestion[len(code):].strip()\n                \n                # Create simple variations for multiple suggestions\n                suggestions = [base_suggestion]\n                if len(base_suggestion) > 10:\n                    # Add truncated versions\n                    suggestions.append(base_suggestion[:len(base_suggestion)//2])\n                    suggestions.append(base_suggestion[:len(base_suggestion)//3])\n                else:\n                    # Add simple variations\n                    suggestions.extend([base_suggestion + \"()\", base_suggestion + \":\"])\n        \n        # Ensure we have the right number of suggestions\n        while len(suggestions) < max_suggestions:\n            suggestions.append(\"\")  # Add empty suggestions if needed\n        \n        return suggestions[:max_suggestions]\n\n    async def process_chat(\n        self,\n        message: str,\n        code_context: Optional[str] = None,\n        language: Optional[str] = None,\n        model_type: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Process a chat message using the fine-tuned CodeGen model.\"\"\"\n        \n        if not self.is_initialized:\n            raise RuntimeError(\"Custom inference engine not initialized\")\n            \n        selected_model = \"codegen\"\n        \n        start_time = time.time()\n        \n        try:\n            model = self.models[selected_model]\n            tokenizer = self.tokenizers[selected_model]\n            \n            response_text = await self._generate_chat_causal(\n                model, tokenizer, message, code_context\n            )\n            \n            processing_time = (time.time() - start_time) * 1000  # Convert to ms\n            \n            return {\n                \"response\": response_text,\n                \"model_used\": f\"custom:{selected_model}\",\n                \"processing_time\": processing_time\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error processing chat with {selected_model}: {e}\")\n            raise\n\n    async def _generate_chat_causal(\n        self,\n        model,\n        tokenizer,\n        message: str,\n        code_context: Optional[str]\n    ) -> str:\n        \"\"\"Generate a chat response using causal language models (CodeGen).\"\"\"\n        \n        prompt = f\"User: {message}\\n\"\n        if code_context:\n            prompt += f\"Code Context:\\n```\\n{code_context}\\n```\\n\"\n        prompt += \"Assistant:\"\n        \n        model.eval()\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        \n        # Handle device placement for Apple Silicon\n        if torch.cuda.is_available():\n            inputs = {k: v.cuda() for k, v in inputs.items()}\n            model = model.cuda()\n        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n            inputs = {k: v.to('mps') for k, v in inputs.items()}\n            model = model.to('mps')\n            \n        try:\n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=100,  # Reduced for stability\n                    num_return_sequences=1,\n                    do_sample=True,\n                    temperature=0.3,  # Lower temperature for stability\n                    top_p=0.9,        # Lower top_p for stability\n                    top_k=50,         # Add top_k for additional stability\n                    pad_token_id=tokenizer.eos_token_id,\n                    repetition_penalty=1.1,  # Prevent repetition\n                    no_repeat_ngram_size=2   # Prevent repetition\n                )\n                \n                response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n                \n                # Clean up response\n                response = response[len(prompt):].strip()\n                \n                return response\n                \n        except Exception as e:\n            logger.warning(f\"Chat generation failed: {e}, falling back to simple response\")\n            return f\"I'm a fine-tuned code model. For '{message}', I'd recommend checking the documentation or trying a different approach.\"\n            \n    def get_status(self) -> str:\n        \"\"\"Get the status of the custom inference engine.\"\"\"\n        if self.is_initialized:\n            return f\"\u2705 Initialized and ready. Default model: {self.default_model}\"\n        return \"\u274c Not initialized.\"\n    \n    def get_model_info(self) -> Dict[str, Any]:\n        \"\"\"Get information about the loaded models.\"\"\"\n        if not self.is_initialized:\n            return {}\n        \n        return {\n            \"loaded_models\": list(self.models.keys()),\n            \"default_model\": self.default_model,\n            \"model_configs\": {\n                name: {\"type\": cfg[\"type\"]} for name, cfg in self.model_configs.items()\n            }\n        }\n    \n    def get_available_models(self) -> List[str]:\n        \"\"\"Get a list of available model types.\"\"\"\n        return list(self.models.keys())\n        \n    def set_default_model(self, model_type: str) -> bool:\n        \"\"\"Set the default model for inference.\"\"\"\n        if model_type in self.models:\n            self.default_model = model_type\n            logger.info(f\"Default model set to: {model_type}\")\n            return True\n        logger.error(f\"Cannot set default model, {model_type} not loaded.\")\n        return False\n        \n    async def cleanup(self):\n        \"\"\"Clean up resources.\"\"\"\n        self.models.clear()\n        self.tokenizers.clear()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        logger.info(\"Custom inference engine cleaned up.\")", "    def __init__(self):\n        self.models = {}\n        self.tokenizers = {}\n        self.model_configs = {}\n        self.default_model = None\n        self.is_initialized = False\n        \n        # Model type mappings\n        self.model_types = {\n            \"codegen\": {\n                \"model_class\": AutoModelForCausalLM,\n                \"tokenizer_class\": AutoTokenizer,\n                \"type\": \"causal_lm\"\n            }\n        }", "    async def initialize(self) -> bool:\n        \"\"\"Initialize the custom inference engine by loading the CodeGen model.\"\"\"\n        logger.info(\"\ud83e\udd16 Initializing custom inference engine...\")\n        \n        try:\n            # Discover the trained CodeGen model\n            model_info = self._discover_trained_model()\n            \n            if not model_info:\n                logger.warning(\"\u26a0\ufe0f No trained CodeGen model found, cannot initialize custom inference\")\n                return False\n            \n            # Load the model\n            if await self._load_model(model_info):\n                self.default_model = \"codegen\"\n                self.is_initialized = True\n                logger.info(\"\u2705 Custom inference engine initialized with CodeGen model\")\n                logger.info(f\"Default model: {self.default_model}\")\n                return True\n            else:\n                logger.error(\"\u274c Failed to load the custom CodeGen model\")\n                return False\n                \n        except Exception as e:\n            logger.error(f\"Failed to initialize custom inference engine: {e}\")\n            return False", "    def _discover_trained_model(self) -> Optional[Dict[str, Any]]:\n        \"\"\"Discover the available trained CodeGen model.\"\"\"\n        checkpoints_dir = Path(\"training/checkpoints\")\n        model_type = \"codegen\"\n        \n        model_path = checkpoints_dir / model_type\n        \n        if model_path.exists() and any(model_path.iterdir()):\n            logger.info(f\"Found trained model: {model_type} at {model_path}\")\n            return {\n                \"model_type\": model_type,\n                \"path\": str(model_path)\n            }\n        return None", "    async def _load_model(self, model_info: Dict[str, Any]) -> bool:\n        \"\"\"Load a specific trained model.\"\"\"\n        model_type = model_info[\"model_type\"]\n        model_path = model_info[\"path\"]\n        \n        logger.info(f\"Loading {model_type} model from {model_path}\")\n        \n        try:\n            # Get model configuration\n            model_config = self.model_types[model_type]\n            \n            # Load tokenizer\n            tokenizer_class = model_config[\"tokenizer_class\"]\n            tokenizer = tokenizer_class.from_pretrained(model_path)\n            \n            # Load model\n            model_class = model_config[\"model_class\"]\n            model = model_class.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                device_map=\"auto\" if torch.cuda.is_available() else None\n            )\n            \n            # Store model and tokenizer\n            self.models[model_type] = model\n            self.tokenizers[model_type] = tokenizer\n            self.model_configs[model_type] = model_config\n            \n            logger.info(f\"\u2705 Successfully loaded {model_type} model\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Failed to load {model_type} model: {e}\")\n            return False", "    async def generate_completions(\n        self,\n        code: str,\n        language: str,\n        max_suggestions: int = 3,\n        cursor_position: Optional[int] = None,\n        model_type: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Generate code completions using the custom CodeGen model.\"\"\"\n        \n        if not self.is_initialized:\n            raise RuntimeError(\"Custom inference engine not initialized\")\n        \n        selected_model = \"codegen\"\n        \n        start_time = time.time()\n        \n        try:\n            model = self.models[selected_model]\n            tokenizer = self.tokenizers[selected_model]\n            \n            suggestions = await self._generate_causal_completions(\n                model, tokenizer, code, max_suggestions\n            )\n            \n            processing_time = (time.time() - start_time) * 1000  # Convert to ms\n            \n            # Calculate confidence scores\n            confidence_scores = [0.9 - (i * 0.1) for i in range(len(suggestions))]\n            \n            return {\n                \"suggestions\": suggestions,\n                \"confidence_scores\": confidence_scores,\n                \"model_used\": f\"custom:{selected_model}\",\n                \"language\": language,\n                \"processing_time\": processing_time\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error generating completions with {selected_model}: {e}\")\n            raise", "    async def _generate_causal_completions(\n        self,\n        model,\n        tokenizer,\n        code: str,\n        max_suggestions: int\n    ) -> List[str]:\n        \"\"\"Generate completions using causal language models (CodeGen).\"\"\"\n        \n        model.eval()\n        \n        # Tokenize input\n        inputs = tokenizer(code, return_tensors=\"pt\")\n        \n        # Handle device placement for Apple Silicon\n        if torch.cuda.is_available():\n            inputs = {k: v.cuda() for k, v in inputs.items()}\n            model = model.cuda()\n        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n            inputs = {k: v.to('mps') for k, v in inputs.items()}\n            model = model.to('mps')\n        \n        suggestions = []\n        \n        try:\n            with torch.no_grad():\n                # Try sampling first with safer parameters\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=30,\n                    num_return_sequences=max_suggestions,\n                    do_sample=True,\n                    temperature=0.1,  # Lower temperature for stability\n                    top_p=0.9,        # Lower top_p for stability\n                    top_k=50,         # Add top_k for additional stability\n                    pad_token_id=tokenizer.eos_token_id,\n                    repetition_penalty=1.1,  # Prevent repetition\n                    no_repeat_ngram_size=2   # Prevent repetition\n                )\n                \n                # Decode and clean suggestions\n                suggestions = [\n                    tokenizer.decode(output, skip_special_tokens=True)\n                    for output in outputs\n                ]\n                \n                # Remove the original code from the suggestion\n                suggestions = [s[len(code):].strip() for s in suggestions]\n                \n        except Exception as e:\n            logger.warning(f\"Sampling generation failed: {e}, falling back to greedy decoding\")\n            \n            # Fallback to greedy decoding (more stable)\n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=30,\n                    num_return_sequences=1,\n                    do_sample=False,  # Greedy decoding\n                    pad_token_id=tokenizer.eos_token_id\n                )\n                \n                # Decode and create variations\n                base_suggestion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n                base_suggestion = base_suggestion[len(code):].strip()\n                \n                # Create simple variations for multiple suggestions\n                suggestions = [base_suggestion]\n                if len(base_suggestion) > 10:\n                    # Add truncated versions\n                    suggestions.append(base_suggestion[:len(base_suggestion)//2])\n                    suggestions.append(base_suggestion[:len(base_suggestion)//3])\n                else:\n                    # Add simple variations\n                    suggestions.extend([base_suggestion + \"()\", base_suggestion + \":\"])\n        \n        # Ensure we have the right number of suggestions\n        while len(suggestions) < max_suggestions:\n            suggestions.append(\"\")  # Add empty suggestions if needed\n        \n        return suggestions[:max_suggestions]", "    async def process_chat(\n        self,\n        message: str,\n        code_context: Optional[str] = None,\n        language: Optional[str] = None,\n        model_type: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Process a chat message using the fine-tuned CodeGen model.\"\"\"\n        \n        if not self.is_initialized:\n            raise RuntimeError(\"Custom inference engine not initialized\")\n            \n        selected_model = \"codegen\"\n        \n        start_time = time.time()\n        \n        try:\n            model = self.models[selected_model]\n            tokenizer = self.tokenizers[selected_model]\n            \n            response_text = await self._generate_chat_causal(\n                model, tokenizer, message, code_context\n            )\n            \n            processing_time = (time.time() - start_time) * 1000  # Convert to ms\n            \n            return {\n                \"response\": response_text,\n                \"model_used\": f\"custom:{selected_model}\",\n                \"processing_time\": processing_time\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error processing chat with {selected_model}: {e}\")\n            raise", "    async def _generate_chat_causal(\n        self,\n        model,\n        tokenizer,\n        message: str,\n        code_context: Optional[str]\n    ) -> str:\n        \"\"\"Generate a chat response using causal language models (CodeGen).\"\"\"\n        \n        prompt = f\"User: {message}\\n\"\n        if code_context:\n            prompt += f\"Code Context:\\n```\\n{code_context}\\n```\\n\"\n        prompt += \"Assistant:\"\n        \n        model.eval()\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        \n        # Handle device placement for Apple Silicon\n        if torch.cuda.is_available():\n            inputs = {k: v.cuda() for k, v in inputs.items()}\n            model = model.cuda()\n        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n            inputs = {k: v.to('mps') for k, v in inputs.items()}\n            model = model.to('mps')\n            \n        try:\n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=100,  # Reduced for stability\n                    num_return_sequences=1,\n                    do_sample=True,\n                    temperature=0.3,  # Lower temperature for stability\n                    top_p=0.9,        # Lower top_p for stability\n                    top_k=50,         # Add top_k for additional stability\n                    pad_token_id=tokenizer.eos_token_id,\n                    repetition_penalty=1.1,  # Prevent repetition\n                    no_repeat_ngram_size=2   # Prevent repetition\n                )\n                \n                response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n                \n                # Clean up response\n                response = response[len(prompt):].strip()\n                \n                return response\n                \n        except Exception as e:\n            logger.warning(f\"Chat generation failed: {e}, falling back to simple response\")\n            return f\"I'm a fine-tuned code model. For '{message}', I'd recommend checking the documentation or trying a different approach.\"", "    def get_status(self) -> str:\n        \"\"\"Get the status of the custom inference engine.\"\"\"\n        if self.is_initialized:\n            return f\"\u2705 Initialized and ready. Default model: {self.default_model}\"\n        return \"\u274c Not initialized.\"", "    def get_model_info(self) -> Dict[str, Any]:\n        \"\"\"Get information about the loaded models.\"\"\"\n        if not self.is_initialized:\n            return {}\n        \n        return {\n            \"loaded_models\": list(self.models.keys()),\n            \"default_model\": self.default_model,\n            \"model_configs\": {\n                name: {\"type\": cfg[\"type\"]} for name, cfg in self.model_configs.items()\n            }\n        }", "    def get_available_models(self) -> List[str]:\n        \"\"\"Get a list of available model types.\"\"\"\n        return list(self.models.keys())", "    def set_default_model(self, model_type: str) -> bool:\n        \"\"\"Set the default model for inference.\"\"\"\n        if model_type in self.models:\n            self.default_model = model_type\n            logger.info(f\"Default model set to: {model_type}\")\n            return True\n        logger.error(f\"Cannot set default model, {model_type} not loaded.\")\n        return False", "    async def cleanup(self):\n        \"\"\"Clean up resources.\"\"\"\n        self.models.clear()\n        self.tokenizers.clear()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        logger.info(\"Custom inference engine cleaned up.\")", "class RAGRetriever:\n    def __init__(self, index_path: Optional[str] = None):\n        self.rag_index = FaissRAGIndex()\n        # Try to load from default path if no specific path provided\n        if index_path is None:\n            index_path = os.path.join(os.path.dirname(__file__), 'index_data')\n        \n        if os.path.exists(index_path):\n            try:\n                self.rag_index.load(index_path)\n                print(f\"\u2705 Loaded RAG index from {index_path}\")\n            except Exception as e:\n                print(f\"\u26a0\ufe0f Failed to load RAG index from {index_path}: {e}\")\n        else:\n            print(f\"\u26a0\ufe0f No RAG index found at {index_path}. Run the indexer first.\")\n\n    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:\n        \"\"\"\n        Retrieve top-k relevant chunks for the query.\n        Returns a list of dicts with 'text', 'metadata', and 'score'.\n        \"\"\"\n        results = self.rag_index.search(query, top_k=top_k)\n        return [\n            {'text': text, 'metadata': meta, 'score': score}\n            for text, meta, score in results\n        ]\n\n    def is_loaded(self) -> bool:\n        \"\"\"Check if the index is loaded and ready.\"\"\"\n        return len(self.rag_index.text_chunks) > 0", "    def __init__(self, index_path: Optional[str] = None):\n        self.rag_index = FaissRAGIndex()\n        # Try to load from default path if no specific path provided\n        if index_path is None:\n            index_path = os.path.join(os.path.dirname(__file__), 'index_data')\n        \n        if os.path.exists(index_path):\n            try:\n                self.rag_index.load(index_path)\n                print(f\"\u2705 Loaded RAG index from {index_path}\")\n            except Exception as e:\n                print(f\"\u26a0\ufe0f Failed to load RAG index from {index_path}: {e}\")\n        else:\n            print(f\"\u26a0\ufe0f No RAG index found at {index_path}. Run the indexer first.\")", "    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:\n        \"\"\"\n        Retrieve top-k relevant chunks for the query.\n        Returns a list of dicts with 'text', 'metadata', and 'score'.\n        \"\"\"\n        results = self.rag_index.search(query, top_k=top_k)\n        return [\n            {'text': text, 'metadata': meta, 'score': score}\n            for text, meta, score in results\n        ]", "    def is_loaded(self) -> bool:\n        \"\"\"Check if the index is loaded and ready.\"\"\"\n        return len(self.rag_index.text_chunks) > 0", "class FaissRAGIndex:\n    def __init__(self, embedding_model_name: str = 'all-MiniLM-L6-v2', embedding_dim: int = 384):\n        self.embedding_model = SentenceTransformer(embedding_model_name)\n        self.embedding_dim = embedding_dim\n        self.index = faiss.IndexFlatL2(embedding_dim)\n        self.text_chunks: List[str] = []\n        self.chunk_metadata: List[dict] = []\n\n    def add_documents(self, documents: List[Tuple[str, dict]]):\n        if not documents:\n            print(\"Warning: No documents provided to index\")\n            return\n        \n        texts = [doc[0] for doc in documents]\n        print(f\"Generating embeddings for {len(texts)} documents...\")\n        embeddings = self.embedding_model.encode(texts, convert_to_numpy=True)\n        self.index.add(embeddings)\n        self.text_chunks.extend(texts)\n        self.chunk_metadata.extend([doc[1] for doc in documents])\n        print(f\"Successfully indexed {len(texts)} documents\")\n\n    def search(self, query: str, top_k: int = 5) -> List[Tuple[str, dict, float]]:\n        query_emb = self.embedding_model.encode([query], convert_to_numpy=True)\n        D, I = self.index.search(query_emb, top_k)\n        results = []\n        for idx, score in zip(I[0], D[0]):\n            if idx < len(self.text_chunks):\n                results.append((self.text_chunks[idx], self.chunk_metadata[idx], float(score)))\n        return results\n\n    def save(self, path: str):\n        os.makedirs(path, exist_ok=True)\n        faiss.write_index(self.index, os.path.join(path, 'faiss.index'))\n        with open(os.path.join(path, 'chunks.json'), 'w', encoding='utf-8') as f:\n            json.dump(self.text_chunks, f)\n        with open(os.path.join(path, 'metadata.json'), 'w', encoding='utf-8') as f:\n            json.dump(self.chunk_metadata, f)\n\n    def load(self, path: str):\n        self.index = faiss.read_index(os.path.join(path, 'faiss.index'))\n        with open(os.path.join(path, 'chunks.json'), 'r', encoding='utf-8') as f:\n            self.text_chunks = json.load(f)\n        with open(os.path.join(path, 'metadata.json'), 'r', encoding='utf-8') as f:\n            self.chunk_metadata = json.load(f) ", "    def __init__(self, embedding_model_name: str = 'all-MiniLM-L6-v2', embedding_dim: int = 384):\n        self.embedding_model = SentenceTransformer(embedding_model_name)\n        self.embedding_dim = embedding_dim\n        self.index = faiss.IndexFlatL2(embedding_dim)\n        self.text_chunks: List[str] = []\n        self.chunk_metadata: List[dict] = []", "    def add_documents(self, documents: List[Tuple[str, dict]]):\n        if not documents:\n            print(\"Warning: No documents provided to index\")\n            return\n        \n        texts = [doc[0] for doc in documents]\n        print(f\"Generating embeddings for {len(texts)} documents...\")\n        embeddings = self.embedding_model.encode(texts, convert_to_numpy=True)\n        self.index.add(embeddings)\n        self.text_chunks.extend(texts)\n        self.chunk_metadata.extend([doc[1] for doc in documents])\n        print(f\"Successfully indexed {len(texts)} documents\")", "    def search(self, query: str, top_k: int = 5) -> List[Tuple[str, dict, float]]:\n        query_emb = self.embedding_model.encode([query], convert_to_numpy=True)\n        D, I = self.index.search(query_emb, top_k)\n        results = []\n        for idx, score in zip(I[0], D[0]):\n            if idx < len(self.text_chunks):\n                results.append((self.text_chunks[idx], self.chunk_metadata[idx], float(score)))\n        return results", "    def save(self, path: str):\n        os.makedirs(path, exist_ok=True)\n        faiss.write_index(self.index, os.path.join(path, 'faiss.index'))\n        with open(os.path.join(path, 'chunks.json'), 'w', encoding='utf-8') as f:\n            json.dump(self.text_chunks, f)\n        with open(os.path.join(path, 'metadata.json'), 'w', encoding='utf-8') as f:\n            json.dump(self.chunk_metadata, f)", "    def load(self, path: str):\n        self.index = faiss.read_index(os.path.join(path, 'faiss.index'))\n        with open(os.path.join(path, 'chunks.json'), 'r', encoding='utf-8') as f:\n            self.text_chunks = json.load(f)\n        with open(os.path.join(path, 'metadata.json'), 'r', encoding='utf-8') as f:\n            self.chunk_metadata = json.load(f) ", "def chunk_markdown(text: str) -> List[str]:\n    # Split by headers, then by paragraphs\n    header_chunks = re.split(r'(^#+ .*$)', text, flags=re.MULTILINE)\n    chunks = []\n    buffer = ''\n    for part in header_chunks:\n        if part.strip().startswith('#'):\n            if buffer.strip():\n                chunks.append(buffer.strip())\n            buffer = part.strip() + '\\n'\n        else:\n            buffer += part\n    if buffer.strip():\n        chunks.extend([p.strip() for p in buffer.split('\\n\\n') if p.strip()])\n    return [c for c in chunks if len(c.split()) > 10]  # Filter very short chunks", "def chunk_python_code(text: str) -> List[Tuple[str, Dict]]:\n    import ast\n    chunks = []\n    try:\n        tree = ast.parse(text)\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n                start = node.lineno - 1\n                end = max([getattr(n, 'end_lineno', start) for n in ast.walk(node)])\n                lines = text.splitlines()[start:end]\n                chunk = '\\n'.join(lines)\n                if len(chunk.split()) > 5:\n                    chunks.append((chunk, {'type': type(node).__name__, 'name': getattr(node, 'name', None), 'start': start, 'end': end}))\n    except Exception:\n        pass\n    return chunks", "def chunk_code_lines(text: str, window: int = 20, overlap: int = 5) -> List[Tuple[str, Dict]]:\n    lines = text.splitlines()\n    chunks = []\n    for i in range(0, len(lines), window - overlap):\n        chunk = '\\n'.join(lines[i:i+window])\n        if len(chunk.split()) > 5:\n            chunks.append((chunk, {'type': 'lines', 'start': i, 'end': i+window}))\n    return chunks", "def index_project(root_dirs: List[str], faiss_index_path: str = None):\n    if faiss_index_path is None:\n        faiss_index_path = os.path.join(os.path.dirname(__file__), 'index_data')\n    \n    rag_index = FaissRAGIndex()\n    documents = []\n    file_count = 0\n    \n    for root_dir in root_dirs:\n        print(f\"Scanning directory: {root_dir}\")\n        if not os.path.exists(root_dir):\n            print(f\"Warning: Directory {root_dir} does not exist, skipping...\")\n            continue\n            \n        for path in Path(root_dir).rglob('*'):\n            if path.is_file():\n                file_count += 1\n                if file_count % 10 == 0:\n                    print(f\"Processed {file_count} files...\")\n                \n                ext = path.suffix.lower()\n                try:\n                    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n                        text = f.read()\n                    \n                    if ext in DOC_EXTENSIONS:\n                        chunks = chunk_markdown(text)\n                        for chunk in chunks:\n                            documents.append((chunk, {'file': str(path), 'type': 'doc'}))\n                        if chunks:\n                            print(f\"  Added {len(chunks)} doc chunks from {path.name}\")\n                            \n                    elif ext == '.py':\n                        code_chunks = chunk_python_code(text)\n                        if not code_chunks:\n                            code_chunks = chunk_code_lines(text)\n                        for chunk, meta in code_chunks:\n                            meta.update({'file': str(path)})\n                            documents.append((chunk, meta))\n                        if code_chunks:\n                            print(f\"  Added {len(code_chunks)} code chunks from {path.name}\")\n                            \n                    elif ext in CODE_EXTENSIONS:\n                        code_chunks = chunk_code_lines(text)\n                        for chunk, meta in code_chunks:\n                            meta.update({'file': str(path)})\n                            documents.append((chunk, meta))\n                        if code_chunks:\n                            print(f\"  Added {len(code_chunks)} code chunks from {path.name}\")\n                            \n                except Exception as e:\n                    print(f\"Error reading {path}: {e}\")\n                    continue\n    \n    print(f\"\\nTotal files processed: {file_count}\")\n    print(f\"Total chunks extracted: {len(documents)}\")\n    \n    if documents:\n        rag_index.add_documents(documents)\n        rag_index.save(faiss_index_path)\n        print(f\"Indexed {len(documents)} chunks and saved to {faiss_index_path}\")\n    else:\n        print(\"No documents found to index!\")\n    \n    return rag_index"]